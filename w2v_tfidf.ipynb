{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTt3_Xl5U4eh",
        "outputId": "f1f59a00-8e69-4f00-bb3f-1f08c10e93c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy3)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "SoGjSA8aUl2w",
        "outputId": "089feaba-5995-4d1b-e716-1918efc88382"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c29de6b1f34f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# from my_parsers import download, parse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import pymorphy3\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
        "# from my_parsers import download, parse\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "# from brown_clustering import BigramCorpus, BrownClustering\n",
        "\n",
        "pd.options.display.max_rows = 200\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "metadata": {
        "id": "Gi-_yXwAXALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация анализатора pymorphy2 для лемматизации\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Функция для лемматизации слова\n",
        "def lemmatize_word(word):\n",
        "    return morph.parse(word)[0].normal_form\n",
        "\n",
        "# Функция для предобработки данных (разбиение на слова, лемматизация, удаление пунктуации)\n",
        "def preprocess_names(company_names):\n",
        "    processed_names = []\n",
        "    for name in tqdm(company_names):\n",
        "        # Приводим название к нижнему регистру\n",
        "        name = name.lower()\n",
        "        # Удаляем всю пунктуацию, оставляем только слова\n",
        "        name = re.sub(r'[^\\w\\s]', '', name)\n",
        "        # Разбиваем на отдельные слова\n",
        "        words = name.split()\n",
        "\n",
        "        # Лемматизируем каждое слово\n",
        "        # lemmatized_words = [lemmatize_word(word) for word in words]\n",
        "\n",
        "        processed_names.append(words)\n",
        "    return processed_names\n",
        "\n",
        "\n",
        "def get_emb_by_modele(model, comp_names_without_job, column_prefix):\n",
        "    all_tokens = set(model.wv.index_to_key)\n",
        "    word_vectors_dict = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "\n",
        "    all_embds = []\n",
        "\n",
        "    for sent in tqdm(comp_names_without_job):\n",
        "        all_emb = [word_vectors_dict[word] for word in sent if word in all_tokens]\n",
        "\n",
        "        if len(all_emb) == 0:\n",
        "            emb = np.zeros(100)\n",
        "        else:\n",
        "            emb = np.mean(all_emb, axis=0)\n",
        "\n",
        "        all_embds.append(emb)\n",
        "    emb_comp_name_df = pd.DataFrame(np.array(all_embds), columns=[f\"{column_prefix}_{i}\" for i in range(100)])\n",
        "    return emb_comp_name_df"
      ],
      "metadata": {
        "id": "C-4XtdG7U2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_emb_by_tfidf(processed_sentences, column_prefix, max_features=100, ):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "    # Преобразуем разреженную матрицу TF-IDF в плотный формат и создаем DataFrame\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f\"{column_prefix}_{word}\" for word in tfidf_vectorizer.get_feature_names_out()])\n",
        "    return tfidf_vectorizer, tfidf_df"
      ],
      "metadata": {
        "id": "HWKkD6aOVFSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "-JGEYbLvWvAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = preprocess_names(company_names)"
      ],
      "metadata": {
        "id": "VEY7jKJ3VbGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_demand = Word2Vec(sentences, vector_size=100, window=8, min_count=1, sg=1)\n",
        "# Сохранение обученной модели\n",
        "model_demand.save(\"demand_word2vec_russian.model\")"
      ],
      "metadata": {
        "id": "9_TSiJWDX0O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_names_without_job = preprocess_names(df_TRAIN_RES_1[\"company_name\"])\n",
        "emb_comp_name_df = get_emb_by_modele(model_comp_name, comp_names_without_job, column_prefix=\"comp_name_emb\")\n",
        "df_TRAIN_RES_1 = pd.concat([df_TRAIN_RES_1.reset_index(drop=True), emb_comp_name_df.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "1im_QkgPVe1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = preprocess_names(df_TRAIN_RES_1[\"demands\"])\n",
        "\n",
        "RETRAIN_MODEL = True\n",
        "column_prefix = \"demnds_tfidf\"\n",
        "\n",
        "if RETRAIN_MODEL:\n",
        "    demands_vectorizer, demands_tfidf = get_emb_by_tfidf([\" \".join(sent) for sent in sentences],\n",
        "                                                     column_prefix=column_prefix)\n",
        "\n",
        "df_TRAIN_RES_1 = pd.concat([df_TRAIN_RES_1.reset_index(drop=True),\n",
        "                            demands_tfidf.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "ipnkUY_7V4o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}