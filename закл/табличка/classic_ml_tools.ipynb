{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TL;DR <br>\n",
    "В коде находятся:\n",
    "* Catboost для разных видов задач + optuna + feats. importance\n",
    "* Xgboost для разных видов задач + optuna\n",
    "* Stacking\n",
    "* Voiting ensemble\n",
    "* Всякая предобработка данных (Fill NaN, feats. selection, etc.)\n",
    "* Другое (Кроссвалидация, подбор трешхолда, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import xgboost\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "import optuna\n",
    "import sklearn\n",
    "import pickle\n",
    "import catboost as cat\n",
    "from utils import set_seed_no_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "set_seed_no_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs(names):\n",
    "    dfs = []\n",
    "    for name in names:\n",
    "        with open(f'data/{name}.pickle', 'rb') as f:\n",
    "            dfs.append(pickle.load(f))\n",
    "    return dfs\n",
    "\n",
    "train, test = load_dfs(['train_processed', 'test_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = sklearn.model_selection.train_test_split(\n",
    "    train,\n",
    "    test_size=0.15,\n",
    "    # stratify=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop(columns=['fare_amount', 'pickup_datetime']).drop(columns=[]), train['fare_amount']\n",
    "X_val, y_val = val.drop(columns=['fare_amount', 'pickup_datetime']), val['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_pool = cat.Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    ")\n",
    "\n",
    "cat_valid_pool = cat.Pool(\n",
    "    data=X_val,\n",
    "    label=y_val,\n",
    ")\n",
    "\n",
    "cat_test_pool = cat.Pool(\n",
    "    data=test.drop(columns=['pickup_datetime']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Catboosts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defult catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catboost params**:\n",
    "* eval_metric {'F1', 'RMSE'}\n",
    "* auto_class_weights {'default', 'Balanced', 'SqrtBalanced'}\n",
    "* text_features {None, text_features}\n",
    "* loss_function {'MultiClass', 'LogLoss', 'CrossEntropy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "cat_classif = catboost.CatBoostClassifier(eval_metric='F1', iterations=1000, random_seed=42, \n",
    "                                             task_type='GPU', auto_class_weights='default')\n",
    "\n",
    "cat_classif.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "             verbose=100, early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.118628\n",
      "0:\tlearn: 7.8717308\ttest: 7.8149516\tbest: 7.8149516 (0)\ttotal: 261ms\tremaining: 4m 21s\n",
      "100:\tlearn: 4.0987639\ttest: 4.0414625\tbest: 4.0414625 (100)\ttotal: 3.34s\tremaining: 29.8s\n",
      "200:\tlearn: 3.9597662\ttest: 3.9494369\tbest: 3.9494340 (199)\ttotal: 6.44s\tremaining: 25.6s\n",
      "300:\tlearn: 3.8804194\ttest: 3.9186903\tbest: 3.9186903 (300)\ttotal: 9.54s\tremaining: 22.2s\n",
      "400:\tlearn: 3.8262360\ttest: 3.9034796\tbest: 3.9034796 (400)\ttotal: 12.7s\tremaining: 19s\n",
      "500:\tlearn: 3.7905484\ttest: 3.8991309\tbest: 3.8991309 (500)\ttotal: 16s\tremaining: 15.9s\n",
      "600:\tlearn: 3.7639580\ttest: 3.8933820\tbest: 3.8933820 (600)\ttotal: 19.2s\tremaining: 12.7s\n",
      "700:\tlearn: 3.7365328\ttest: 3.8858852\tbest: 3.8858602 (698)\ttotal: 22.1s\tremaining: 9.44s\n",
      "800:\tlearn: 3.7159472\ttest: 3.8825382\tbest: 3.8820889 (788)\ttotal: 25.2s\tremaining: 6.26s\n",
      "900:\tlearn: 3.6950012\ttest: 3.8773762\tbest: 3.8773762 (900)\ttotal: 28s\tremaining: 3.08s\n",
      "999:\tlearn: 3.6766270\ttest: 3.8734903\tbest: 3.8731503 (994)\ttotal: 30.9s\tremaining: 0us\n",
      "bestTest = 3.873150268\n",
      "bestIteration = 994\n",
      "Shrink model to first 995 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1b51716be90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regressor\n",
    "cat_reg = catboost.CatBoostRegressor(\n",
    "    eval_metric='RMSE', \n",
    "    iterations=1000, \n",
    "    random_seed=SEED, \n",
    "    task_type='GPU',\n",
    ")\n",
    "\n",
    "cat_reg.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=100, \n",
    "    early_stopping_rounds=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.concat(\n",
    "    [\n",
    "        pd.read_csv('test (1).csv')['key'],\n",
    "        pd.Series(cat_reg.predict(cat_test_pool)).rename('fare_amount'),\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "sub.to_csv('taxi_second.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:41:07,386] A new study created in memory with name: no-name-9e4d7e8e-4805-4123-9bfb-ae5f8da8e562\n",
      "[W 2024-11-12 23:41:07,398] Trial 0 failed with parameters: {'iterations': 1953, 'objective': 'Logloss', 'colsample_bylevel': 0.05730051197511123, 'learning_rate': 0.04230509319887437, 'depth': 6, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 15.449299899277744, 'auto_class_weights': 'SqrtBalanced'} because of the following error: CatBoostError('C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/loss_description.cpp:18: SqrtBalanced loss is not supported').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Mi\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mi\\AppData\\Local\\Temp\\ipykernel_24064\\4127364583.py\", line 20, in objective\n",
      "    cat_cls.fit(X_train, y_train, eval_set=[(X_val, y_val)] ,verbose=0, early_stopping_rounds=500)\n",
      "  File \"C:\\Users\\Mi\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py\", line 5098, in fit\n",
      "    CatBoostClassifier._check_is_compatible_loss(params['loss_function'])\n",
      "  File \"C:\\Users\\Mi\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py\", line 5449, in _check_is_compatible_loss\n",
      "    if isinstance(loss_function, str) and not CatBoost._is_classification_objective(loss_function):\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mi\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py\", line 1872, in _is_classification_objective\n",
      "    return isinstance(loss_function, str) and is_classification_objective(loss_function)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_catboost.pyx\", line 6021, in _catboost.is_classification_objective\n",
      "  File \"_catboost.pyx\", line 6022, in _catboost.is_classification_objective\n",
      "_catboost.CatBoostError: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/loss_description.cpp:18: SqrtBalanced loss is not supported\n",
      "[W 2024-11-12 23:41:07,460] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/loss_description.cpp:18: SqrtBalanced loss is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     27\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(objective, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7200\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)))\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[0;32m     63\u001b[0m             study,\n\u001b[0;32m     64\u001b[0m             func,\n\u001b[0;32m     65\u001b[0m             n_trials,\n\u001b[0;32m     66\u001b[0m             timeout,\n\u001b[0;32m     67\u001b[0m             catch,\n\u001b[0;32m     68\u001b[0m             callbacks,\n\u001b[0;32m     69\u001b[0m             gc_after_trial,\n\u001b[0;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     16\u001b[0m     param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_class_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqrtBalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     18\u001b[0m cat_cls \u001b[38;5;241m=\u001b[39m catboost\u001b[38;5;241m.\u001b[39mCatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m cat_cls\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)] ,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     22\u001b[0m preds \u001b[38;5;241m=\u001b[39m cat_cls\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, preds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:5098\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5096\u001b[0m _process_synonyms(params)\n\u001b[0;32m   5097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m-> 5098\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m   5100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline, use_best_model,\n\u001b[0;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:5449\u001b[0m, in \u001b[0;36mCatBoostClassifier._check_is_compatible_loss\u001b[1;34m(loss_function)\u001b[0m\n\u001b[0;32m   5447\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   5448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_is_compatible_loss\u001b[39m(loss_function):\n\u001b[1;32m-> 5449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_function, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CatBoost\u001b[38;5;241m.\u001b[39m_is_classification_objective(loss_function):\n\u001b[0;32m   5450\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid loss_function=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: for classifier use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5451\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogloss, CrossEntropy, MultiClass, MultiClassOneVsAll or custom objective object\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss_function))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:1872\u001b[0m, in \u001b[0;36m_CatBoostBase._is_classification_objective\u001b[1;34m(loss_function)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   1871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_classification_objective\u001b[39m(loss_function):\n\u001b[1;32m-> 1872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_function, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_classification_objective(loss_function)\n",
      "File \u001b[1;32m_catboost.pyx:6021\u001b[0m, in \u001b[0;36m_catboost.is_classification_objective\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:6022\u001b[0m, in \u001b[0;36m_catboost.is_classification_objective\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/loss_description.cpp:18: SqrtBalanced loss is not supported"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 5000),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "        }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 20)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "    if param[\"objective\"] == \"Logloss\":\n",
    "        param[\"objective\"] = trial.suggest_categorical(\"auto_class_weights\", [\"default\", \"Balanced\", \"SqrtBalanced\"])\n",
    "        \n",
    "    cat_cls = catboost.CatBoostClassifier(**param, eval_metric='F1')\n",
    "\n",
    "    cat_cls.fit(X_train, y_train, eval_set=[(X_val, y_val)] ,verbose=0, early_stopping_rounds=500)\n",
    "    \n",
    "    preds = cat_cls.predict(X_test)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    return f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, timeout=7200)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = sorted(study.trials, key=lambda trial: -trial.value)\n",
    "top_10_trials = sorted_trials[:50] \n",
    "top_trials_params = []\n",
    "for trial in top_10_trials:\n",
    "    top_trials_params.append(trial.params)\n",
    "    print(f\"Trial number: {trial.number}\")\n",
    "    print(f\"Parameters: {trial.params}\")\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(f\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = catboost.get_feature_importance()\n",
    "feature_names = X_train.columns\n",
    "names_more_treshold = []\n",
    "top_feats = []\n",
    "cnt = 0\n",
    "# # Display feature importance\n",
    "for importance, name in sorted(list(zip(feature_importance, feature_names))):\n",
    "    if importance >= 0.05:\n",
    "        cnt += 1\n",
    "        print(f\"Feature: {name}, Importance: {importance:.2f}\")\n",
    "        names_more_treshold.append(name)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat_classif\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "for i in range(0, len(X_train), batch_size):\n",
    "    X_batch = X_train[i:i + batch_size]\n",
    "    y_batch = y_train[i:i + batch_size]\n",
    "    \n",
    "    model.fit(X_batch, y_batch, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    model = catboost.CatBoostClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    \n",
    "    model.save_model(f'catboost_model_{len(models)}.cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destribiution = pd.DataFrame('your_file.csv',  usecols=['Target'])\n",
    "n_splits = 10 # столько чтобы не полетела оперативка\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "id = [[x] for x in range(len(destribiution))]\n",
    "\n",
    "for _, chunk_index in skf.split(id, destribiution):\n",
    "    df_chunk = pd.read_csv('your_file.csv', skiprows=lambda x: x not in id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Xgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defult xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "xgb_classif = xgboost.XGBClassifier(random_state=42, tree_method = 'gpu_hist', device='CUDA')\n",
    "xgb_classif.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "xgb_reg = xgboost.XGBRegressor(random_state=42, tree_method = 'gpu_hist', device='CUDA')\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"binary:logistic\"]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.01, 1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 100.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 100.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1, 10),\n",
    "        'max_delta_step': trial.suggest_float('max_delta_step', 0.0, 10.0),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'device': 'cuda',  \n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    xgb_cls = xgb.XGBClassifier(**param, eval_metric='f1')\n",
    "\n",
    "    xgb_cls.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0, early_stopping_rounds=500)\n",
    "    \n",
    "    preds = xgb_cls.predict(X_test)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    return f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, timeout=7200)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Код грязный, но пока так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DjStacking(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Стэкинг моделей scikit-learn\"\"\"\n",
    "\n",
    "    def __init__(self, models, ens_model):\n",
    "        \"\"\"\n",
    "        Инициализация\n",
    "        models - базовые модели для стекинга\n",
    "        ens_model - мета-модель\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.ens_model = ens_model\n",
    "        self.n = len(models)\n",
    "        self.valid = None\n",
    "        \n",
    "    def fit(self, X, y=None, p=0.25, cv=3, err=0.001, random_state=None, dop_models=None):\n",
    "        \"\"\"\n",
    "        Обучение стекинга\n",
    "        p - в каком отношении делить на обучение / тест\n",
    "            если p = 0 - используем всё обучение!\n",
    "        cv  (при p=0) - сколько фолдов использовать\n",
    "        err (при p=0) - величина случайной добавки к метапризнакам\n",
    "        random_state - инициализация генератора\n",
    "            \n",
    "        \"\"\"\n",
    "        if (p > 0): # делим на обучение и тест\n",
    "            # разбиение на обучение моделей и метамодели\n",
    "            train, valid, y_train, y_valid = train_test_split(X, y, test_size=p, random_state=random_state)\n",
    "            \n",
    "            # заполнение матрицы для обучения метамодели\n",
    "            self.valid = np.zeros((valid.shape[0], self.n))\n",
    "            for t, clf in enumerate(self.models):\n",
    "                clf.fit(train, y_train)\n",
    "                self.valid[:, t] = clf.predict_proba(valid)[:, 1]\n",
    "                \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y_valid)\n",
    "            print(f'F1: {round(f1_score(self.ens_model.predict(self.valid), y_valid), 3)}')\n",
    "\n",
    "        else: # используем всё обучение\n",
    "        \n",
    "            # для регуляризации - берём случайные добавки\n",
    "            self.valid = err*np.random.randn(X.shape[0], self.n)\n",
    "            \n",
    "            for t, clf in enumerate(self.models):\n",
    "                # это oob-ответы алгоритмов\n",
    "                self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "                # но сам алгоритм надо настроить\n",
    "                clf.fit(X, y)\n",
    "            \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y)\n",
    "            print(f'F1: {round(f1_score(self.ens_model.predict(self.valid), y), 3)}')\n",
    "        \n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Работа стэкинга\n",
    "        \"\"\"\n",
    "        # заполение матрицы для мета-классификатора\n",
    "        X_meta = np.zeros((X.shape[0], self.n))\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            X_meta[:, t] = clf.predict(X)\n",
    "        \n",
    "        a = self.ens_model.predict(X_meta)\n",
    "        \n",
    "        return (a)\n",
    "    \n",
    "    def predict_proba(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Работа стэкинга\n",
    "        \"\"\"\n",
    "        # заполение матрицы для мета-классификатора\n",
    "        X_meta = np.zeros((X.shape[0], self.n))\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            X_meta[:, t] = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "        a = self.ens_model.predict_proba(X_meta)\n",
    "        \n",
    "        return (a)\n",
    "    \n",
    "    def fit_ens_model(self, X, y=None, cv=3, err=0.001):\n",
    "        self.valid = err*np.random.randn(X.shape[0], self.n)\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, n_jobs=-1, method='predict_proba')[:, 1]\n",
    "\n",
    "        # Полиномиальные признаки до второй степени\n",
    "\n",
    "        self.ens_model.fit(self.valid, y)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# базовые модели для стекинга\n",
    "\n",
    "'''\n",
    "gbm1 = lgb.LGBMClassifier(random_state=54, device=\"gpu\", learning_rate=0.143)\n",
    "\n",
    "gbm2 = lgb.LGBMClassifier(random_state=8743, device=\"gpu\", learning_rate=0.1)    \n",
    "\n",
    "gbm3 = lgb.LGBMClassifier(random_state=2367, device=\"gpu\", learning_rate=0.3)\n",
    "\n",
    "xgb1 = XGBClassifier(random_state=13, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.15)\n",
    "\n",
    "xgb2 = XGBClassifier(random_state=74, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.1)\n",
    "\n",
    "xgb3 = XGBClassifier(random_state=788, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.19)\n",
    "'''\n",
    "cat1 = catboost.CatBoostRegressor(random_seed=42, verbose=200, eval_metric='RMSE', task_type=\"GPU\")\n",
    "\n",
    "cat2 = catboost.CatBoostRegressor(random_seed=472, verbose=200, eval_metric='RMSE', task_type=\"GPU\")\n",
    "\n",
    "cat3 = catboost.CatBoostRegressor(random_seed=12, verbose=200, eval_metric='RMSE', task_type=\"GPU\")\n",
    "\n",
    "cat4 = catboost.CatBoostRegressor(random_seed=125, verbose=200, eval_metric='RMSE', task_type=\"GPU\")\n",
    "\n",
    "cat5 = catboost.CatBoostRegressor(random_seed=132, verbose=200, eval_metric='RMSE', task_type=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [gbm1, gbm2, gbm3, xgb1, xgb2, xgb3, cat1, cat2, cat3]\n",
    "models = [cat1, cat2, cat3, cat4, cat5]\n",
    "ens_model = catboost.CatBoostClassifier(verbose=200, task_type=\"GPU\", random_seed=62)\n",
    "\n",
    "s2 = DjStacking(models, ens_model)\n",
    "s2.fit(X_train, y_train, p=-1, cv=5, random_state=42)\n",
    "#print(f'F1: {round(f1_score(y_test, preds), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross_val emsemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_trials_params - лучшее из optuna\n",
    "top_trials_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "data = X_train\n",
    "metrics_stratified = []\n",
    "cv_models = []\n",
    "indx = 0\n",
    "for train_index, test_index in skf.split(data, y_train):\n",
    "    x_train_fold, x_test_fold = data.iloc[train_index, :], data.iloc[test_index, :]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    cat_cv = catboost.CatBoostClassifier(**top_trials_params[indx], eval_metric='F1')\n",
    "    cat_cv.fit(x_train_fold, y_train_fold,\n",
    "             verbose=100, early_stopping_rounds=500)\n",
    "    pred = cat_cv.predict(x_test_fold)\n",
    "    metrics_stratified.append((f1_score(pred, y_test_fold).round(3), roc_auc_score(pred, y_test_fold).round(3)))\n",
    "    cv_models.append(cat_cv)\n",
    "    indx += 1\n",
    "\n",
    "print('\\n'.join(map(str, metrics_stratified)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для большего  кол-ва моделей, для которых фолдов уже не хватает\n",
    "# Здесь несколько моделей обучаются на одинаковых фолдах\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "data = X_train\n",
    "metrics_stratified = []\n",
    "cv_models = []\n",
    "indx = 0\n",
    "for train_index, test_index in skf.split(data, y_train):\n",
    "    x_train_fold, x_test_fold = data.iloc[train_index, :], data.iloc[test_index, :]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    catboost1 = catboost.CatBoostClassifier(**top_trials_params[indx], eval_metric='F1')\n",
    "    catboost2 = catboost.CatBoostClassifier(**top_trials_params[indx + 1], eval_metric='F1')\n",
    "    catboost3 = catboost.CatBoostClassifier(**top_trials_params[indx + 2], eval_metric='F1')\n",
    "\n",
    "    catboost1.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500),\n",
    "    catboost2.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500)\n",
    "    catboost3.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500)\n",
    "\n",
    "    pred1, pred2, pred3 = catboost1.predict(x_test_fold), catboost2.predict(x_test_fold), catboost3.predict(x_test_fold)\n",
    "    metrics_stratified.append((f1_score(pred1, y_test_fold).round(3), roc_auc_score(pred1, y_test_fold).round(3)))\n",
    "    metrics_stratified.append((f1_score(pred2, y_test_fold).round(3), roc_auc_score(pred2, y_test_fold).round(3)))\n",
    "    metrics_stratified.append((f1_score(pred3, y_test_fold).round(3), roc_auc_score(pred3, y_test_fold).round(3)))\n",
    "    cv_models.append(catboost1)\n",
    "    cv_models.append(catboost2)\n",
    "    cv_models.append(catboost3)\n",
    "    indx += 3\n",
    "\n",
    "print('\\n'.join(map(str, metrics_stratified)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc = 0\n",
    "mean_f1 = 0\n",
    "for metric in metrics_stratified:\n",
    "    mean_auc += metric[1]\n",
    "    mean_f1 += metric[0]\n",
    "\n",
    "print('ROC_AUC:', (mean_auc / len(metrics_stratified)).round(3))\n",
    "print('F1:', (mean_f1 / len(metrics_stratified)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Val predict\n",
    "preds = []\n",
    "for model in cv_models:\n",
    "    pred = model.predict_proba(X_test)[:,1]\n",
    "    preds.append(pred)\n",
    "\n",
    "arr_np = np.array(preds)\n",
    "mean_arr = np.mean(arr_np, axis=0)\n",
    "\n",
    "pred = (mean_arr >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_mice(trainX):\n",
    "    scaler = StandardScaler()\n",
    "    trainX_mice = trainX.copy()\n",
    "    trainX_mice = pd.DataFrame(scaler.fit_transform(trainX_mice), columns = trainX.columns)\n",
    "    mice_imputer = IterativeImputer(initial_strategy = 'mean',\n",
    "                                    estimator = LinearRegression(n_jobs=-1),\n",
    "                                    random_state = 42, verbose=2, max_iter=10)\n",
    "\n",
    "    mice = mice_imputer.fit_transform(trainX_mice)\n",
    "    return pd.DataFrame(scaler.inverse_transform(mice), columns = trainX.columns), mice_imputer, scaler\n",
    "\n",
    "\n",
    "# new dataframe\n",
    "trainX_mice, mice_imputer, scaler = fillna_mice(X_train)\n",
    "trainX_mice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_simple = imputer.fit_transform(X_train)\n",
    "X_simple = pd.DataFrame(X_simple, columns = X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = X_train[top_feats] # Предварительно отобрать топ\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "print('Polynomial Features shape: ', poly_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# статичные фичи\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "columns_df = X_train.columns\n",
    "sel.fit(X_train)\n",
    "get_sup_col = sel.get_support()\n",
    "\n",
    "# мультикор.\n",
    "df_transformed = sel.transform(X_train)\n",
    "df_transformed = pd.DataFrame(df_transformed, columns=columns_df[get_sup_col])\n",
    "corr_matrix = df_transformed.corr()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "df_transformed = df_transformed.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Disbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Если метрика F1, то лучше будет просто использовать class_weights + подбор трешхолда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Применение увеличения выборки к данным\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# Применение уменьшения выборки к данным\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Применение SMOTE к данным\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cat_reg.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, pred)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(fscore)]\n",
    "print(f\"Best threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost\n",
    "pred = model.predict(X_val)\n",
    "# pred = [1 if x >= 0.5 else 0 for x in pred]\n",
    "print(classification_report(pred, y_val))\n",
    "print(f'F1_score: {f1_score(pred, y_val).round(3)}')\n",
    "print(f'Roc_auc: {roc_auc_score(pred, y_val).round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified cross-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat_classif\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Стратифицированная кросс-валидация F1-метрика (macro):\", cv_scores)\n",
    "print(\"Среднее значение F1:\", np.mean(cv_scores))\n",
    "print(\"Стандартное отклонение F1:\", np.std(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
