{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TL;DR <br>\n",
    "В коде находятся:\n",
    "* Catboost для разных видов задач + optuna + feats. importance\n",
    "* Xgboost для разных видов задач + optuna\n",
    "* Stacking\n",
    "* Voiting ensemble\n",
    "* Всякая предобработка данных (Fill NaN, feats. selection, etc.)\n",
    "* Другое (Кроссвалидация, подбор трешхолда, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import xgboost\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your df\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = 0, 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Catboosts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defult catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catboost params**:\n",
    "* eval_metric {'F1', 'RMSE'}\n",
    "* auto_class_weights {'default', 'Balanced', 'SqrtBalanced'}\n",
    "* text_features {None, text_features}\n",
    "* loss_function {'MultiClass', 'LogLoss', 'CrossEntropy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "cat_classif = catboost.CatBoostClassifier(eval_metric='F1', iterations=1000, random_seed=42, \n",
    "                                             task_type='GPU', auto_class_weights='default')\n",
    "\n",
    "cat_classif.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "             verbose=100, early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor\n",
    "cat_reg = catboost.CatBoostRegressor(eval_metric='RMSE', iterations=1000, random_seed=42, \n",
    "                                             task_type='GPU', auto_class_weights='default')\n",
    "\n",
    "cat_reg.fit(X_train, y_train, eval_set=(X_val, y_val),\n",
    "             verbose=100, early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 5000),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "        }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 20)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "    if param[\"objective\"] == \"Logloss\":\n",
    "        param[\"objective\"] = trial.suggest_categorical(\"auto_class_weights\", [\"default\", \"Balanced\", \"SqrtBalanced\"])\n",
    "        \n",
    "    cat_cls = catboost.CatBoostClassifier(**param, eval_metric='F1')\n",
    "\n",
    "    cat_cls.fit(X_train, y_train, eval_set=[(X_val, y_val)] ,verbose=0, early_stopping_rounds=500)\n",
    "    \n",
    "    preds = cat_cls.predict(X_test)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    return f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, timeout=7200)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = sorted(study.trials, key=lambda trial: -trial.value)\n",
    "top_10_trials = sorted_trials[:50] \n",
    "top_trials_params = []\n",
    "for trial in top_10_trials:\n",
    "    top_trials_params.append(trial.params)\n",
    "    print(f\"Trial number: {trial.number}\")\n",
    "    print(f\"Parameters: {trial.params}\")\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(f\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = catboost.get_feature_importance()\n",
    "feature_names = X_train.columns\n",
    "names_more_treshold = []\n",
    "top_feats = []\n",
    "cnt = 0\n",
    "# # Display feature importance\n",
    "for importance, name in sorted(list(zip(feature_importance, feature_names))):\n",
    "    if importance >= 0.05:\n",
    "        cnt += 1\n",
    "        print(f\"Feature: {name}, Importance: {importance:.2f}\")\n",
    "        names_more_treshold.append(name)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat_classif\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "for i in range(0, len(X_train), batch_size):\n",
    "    X_batch = X_train[i:i + batch_size]\n",
    "    y_batch = y_train[i:i + batch_size]\n",
    "    \n",
    "    model.fit(X_batch, y_batch, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    model = catboost.CatBoostClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    \n",
    "    model.save_model(f'catboost_model_{len(models)}.cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destribiution = pd.DataFrame('your_file.csv',  usecols=['Target'])\n",
    "n_splits = 10 # столько чтобы не полетела оперативка\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "id = [[x] for x in range(len(destribiution))]\n",
    "\n",
    "for _, chunk_index in skf.split(id, destribiution):\n",
    "    df_chunk = pd.read_csv('your_file.csv', skiprows=lambda x: x not in id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Xgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defult xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "xgb_classif = xgboost.XGBClassifier(random_state=42, tree_method = 'gpu_hist', device='CUDA')\n",
    "xgb_classif.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "xgb_reg = xgboost.XGBRegressor(random_state=42, tree_method = 'gpu_hist', device='CUDA')\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"binary:logistic\"]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.01, 1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 100.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 100.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1, 10),\n",
    "        'max_delta_step': trial.suggest_float('max_delta_step', 0.0, 10.0),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'device': 'cuda',  \n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    xgb_cls = xgb.XGBClassifier(**param, eval_metric='f1')\n",
    "\n",
    "    xgb_cls.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0, early_stopping_rounds=500)\n",
    "    \n",
    "    preds = xgb_cls.predict(X_test)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    return f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, timeout=7200)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Код грязный, но пока так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DjStacking(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Стэкинг моделей scikit-learn\"\"\"\n",
    "\n",
    "    def __init__(self, models, ens_model):\n",
    "        \"\"\"\n",
    "        Инициализация\n",
    "        models - базовые модели для стекинга\n",
    "        ens_model - мета-модель\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.ens_model = ens_model\n",
    "        self.n = len(models)\n",
    "        self.valid = None\n",
    "        \n",
    "    def fit(self, X, y=None, p=0.25, cv=3, err=0.001, random_state=None, dop_models=None):\n",
    "        \"\"\"\n",
    "        Обучение стекинга\n",
    "        p - в каком отношении делить на обучение / тест\n",
    "            если p = 0 - используем всё обучение!\n",
    "        cv  (при p=0) - сколько фолдов использовать\n",
    "        err (при p=0) - величина случайной добавки к метапризнакам\n",
    "        random_state - инициализация генератора\n",
    "            \n",
    "        \"\"\"\n",
    "        if (p > 0): # делим на обучение и тест\n",
    "            # разбиение на обучение моделей и метамодели\n",
    "            train, valid, y_train, y_valid = train_test_split(X, y, test_size=p, random_state=random_state)\n",
    "            \n",
    "            # заполнение матрицы для обучения метамодели\n",
    "            self.valid = np.zeros((valid.shape[0], self.n))\n",
    "            for t, clf in enumerate(self.models):\n",
    "                clf.fit(train, y_train)\n",
    "                self.valid[:, t] = clf.predict_proba(valid)[:, 1]\n",
    "                \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y_valid)\n",
    "            print(f'F1: {round(f1_score(self.ens_model.predict(self.valid), y_valid), 3)}')\n",
    "\n",
    "        else: # используем всё обучение\n",
    "        \n",
    "            # для регуляризации - берём случайные добавки\n",
    "            self.valid = err*np.random.randn(X.shape[0], self.n)\n",
    "            \n",
    "            for t, clf in enumerate(self.models):\n",
    "                # это oob-ответы алгоритмов\n",
    "                self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "                # но сам алгоритм надо настроить\n",
    "                clf.fit(X, y)\n",
    "            \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y)\n",
    "            print(f'F1: {round(f1_score(self.ens_model.predict(self.valid), y), 3)}')\n",
    "        \n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Работа стэкинга\n",
    "        \"\"\"\n",
    "        # заполение матрицы для мета-классификатора\n",
    "        X_meta = np.zeros((X.shape[0], self.n))\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            X_meta[:, t] = clf.predict(X)\n",
    "        \n",
    "        a = self.ens_model.predict(X_meta)\n",
    "        \n",
    "        return (a)\n",
    "    \n",
    "    def predict_proba(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Работа стэкинга\n",
    "        \"\"\"\n",
    "        # заполение матрицы для мета-классификатора\n",
    "        X_meta = np.zeros((X.shape[0], self.n))\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            X_meta[:, t] = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "        a = self.ens_model.predict_proba(X_meta)\n",
    "        \n",
    "        return (a)\n",
    "    \n",
    "    def fit_ens_model(self, X, y=None, cv=3, err=0.001):\n",
    "        self.valid = err*np.random.randn(X.shape[0], self.n)\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, n_jobs=-1, method='predict_proba')[:, 1]\n",
    "\n",
    "        # Полиномиальные признаки до второй степени\n",
    "\n",
    "        self.ens_model.fit(self.valid, y)  \n",
    "        \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# базовые модели для стекинга\n",
    "\n",
    "'''\n",
    "gbm1 = lgb.LGBMClassifier(random_state=54, device=\"gpu\", learning_rate=0.143)\n",
    "\n",
    "gbm2 = lgb.LGBMClassifier(random_state=8743, device=\"gpu\", learning_rate=0.1)    \n",
    "\n",
    "gbm3 = lgb.LGBMClassifier(random_state=2367, device=\"gpu\", learning_rate=0.3)\n",
    "\n",
    "xgb1 = XGBClassifier(random_state=13, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.15)\n",
    "\n",
    "xgb2 = XGBClassifier(random_state=74, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.1)\n",
    "\n",
    "xgb3 = XGBClassifier(random_state=788, tree_method = 'gpu_hist', device='CUDA', learning_rate=0.19)\n",
    "'''\n",
    "cat1 = catboost.CatBoostClassifier(random_seed=42, verbose=200, eval_metric='F1', task_type=\"GPU\")\n",
    "\n",
    "cat2 = catboost.CatBoostClassifier(random_seed=472, verbose=200, eval_metric='F1', task_type=\"GPU\")\n",
    "\n",
    "cat3 = catboost.CatBoostClassifier(random_seed=12, verbose=200, eval_metric='F1', task_type=\"GPU\")\n",
    "\n",
    "cat4 = catboost.CatBoostClassifier(random_seed=125, verbose=200, eval_metric='F1', task_type=\"GPU\")\n",
    "\n",
    "cat5 = catboost.CatBoostClassifier(random_seed=132, verbose=200, eval_metric='F1', task_type=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [gbm1, gbm2, gbm3, xgb1, xgb2, xgb3, cat1, cat2, cat3]\n",
    "models = [cat1, cat2, cat3, cat4, cat5]\n",
    "ens_model = catboost.CatBoostClassifier(verbose=200, task_type=\"GPU\", random_seed=62)\n",
    "\n",
    "s2 = DjStacking(models, ens_model)\n",
    "s2.fit(X_train, y_train, p=-1, cv=5, random_state=42)\n",
    "#print(f'F1: {round(f1_score(y_test, preds), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross_val emsemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_trials_params - лучшее из optuna\n",
    "top_trials_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "data = X_train\n",
    "metrics_stratified = []\n",
    "cv_models = []\n",
    "indx = 0\n",
    "for train_index, test_index in skf.split(data, y_train):\n",
    "    x_train_fold, x_test_fold = data.iloc[train_index, :], data.iloc[test_index, :]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    cat_cv = catboost.CatBoostClassifier(**top_trials_params[indx], eval_metric='F1')\n",
    "    cat_cv.fit(x_train_fold, y_train_fold,\n",
    "             verbose=100, early_stopping_rounds=500)\n",
    "    pred = cat_cv.predict(x_test_fold)\n",
    "    metrics_stratified.append((f1_score(pred, y_test_fold).round(3), roc_auc_score(pred, y_test_fold).round(3)))\n",
    "    cv_models.append(cat_cv)\n",
    "    indx += 1\n",
    "\n",
    "print('\\n'.join(map(str, metrics_stratified)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для большего  кол-ва моделей, для которых фолдов уже не хватает\n",
    "# Здесь несколько моделей обучаются на одинаковых фолдах\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "data = X_train\n",
    "metrics_stratified = []\n",
    "cv_models = []\n",
    "indx = 0\n",
    "for train_index, test_index in skf.split(data, y_train):\n",
    "    x_train_fold, x_test_fold = data.iloc[train_index, :], data.iloc[test_index, :]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    catboost1 = catboost.CatBoostClassifier(**top_trials_params[indx], eval_metric='F1')\n",
    "    catboost2 = catboost.CatBoostClassifier(**top_trials_params[indx + 1], eval_metric='F1')\n",
    "    catboost3 = catboost.CatBoostClassifier(**top_trials_params[indx + 2], eval_metric='F1')\n",
    "\n",
    "    catboost1.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500),\n",
    "    catboost2.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500)\n",
    "    catboost3.fit(x_train_fold, y_train_fold, verbose=300, early_stopping_rounds=500)\n",
    "\n",
    "    pred1, pred2, pred3 = catboost1.predict(x_test_fold), catboost2.predict(x_test_fold), catboost3.predict(x_test_fold)\n",
    "    metrics_stratified.append((f1_score(pred1, y_test_fold).round(3), roc_auc_score(pred1, y_test_fold).round(3)))\n",
    "    metrics_stratified.append((f1_score(pred2, y_test_fold).round(3), roc_auc_score(pred2, y_test_fold).round(3)))\n",
    "    metrics_stratified.append((f1_score(pred3, y_test_fold).round(3), roc_auc_score(pred3, y_test_fold).round(3)))\n",
    "    cv_models.append(catboost1)\n",
    "    cv_models.append(catboost2)\n",
    "    cv_models.append(catboost3)\n",
    "    indx += 3\n",
    "\n",
    "print('\\n'.join(map(str, metrics_stratified)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc = 0\n",
    "mean_f1 = 0\n",
    "for metric in metrics_stratified:\n",
    "    mean_auc += metric[1]\n",
    "    mean_f1 += metric[0]\n",
    "\n",
    "print('ROC_AUC:', (mean_auc / len(metrics_stratified)).round(3))\n",
    "print('F1:', (mean_f1 / len(metrics_stratified)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Val predict\n",
    "preds = []\n",
    "for model in cv_models:\n",
    "    pred = model.predict_proba(X_test)[:,1]\n",
    "    preds.append(pred)\n",
    "\n",
    "arr_np = np.array(preds)\n",
    "mean_arr = np.mean(arr_np, axis=0)\n",
    "\n",
    "pred = (mean_arr >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_mice(trainX):\n",
    "    scaler = StandardScaler()\n",
    "    trainX_mice = trainX.copy()\n",
    "    trainX_mice = pd.DataFrame(scaler.fit_transform(trainX_mice), columns = trainX.columns)\n",
    "    mice_imputer = IterativeImputer(initial_strategy = 'mean',\n",
    "                                    estimator = LinearRegression(n_jobs=-1),\n",
    "                                    random_state = 42, verbose=2, max_iter=10)\n",
    "\n",
    "    mice = mice_imputer.fit_transform(trainX_mice)\n",
    "    return pd.DataFrame(scaler.inverse_transform(mice), columns = trainX.columns), mice_imputer, scaler\n",
    "\n",
    "\n",
    "# new dataframe\n",
    "trainX_mice, mice_imputer, scaler = fillna_mice(X_train)\n",
    "trainX_mice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_simple = imputer.fit_transform(X_train)\n",
    "X_simple = pd.DataFrame(X_simple, columns = X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = X_train[top_feats] # Предварительно отобрать топ\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "print('Polynomial Features shape: ', poly_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# статичные фичи\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "columns_df = X_train.columns\n",
    "sel.fit(X_train)\n",
    "get_sup_col = sel.get_support()\n",
    "\n",
    "# мультикор.\n",
    "df_transformed = sel.transform(X_train)\n",
    "df_transformed = pd.DataFrame(df_transformed, columns=columns_df[get_sup_col])\n",
    "corr_matrix = df_transformed.corr()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "df_transformed = df_transformed.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Disbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Если метрика F1, то лучше будет просто использовать class_weights + подбор трешхолда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Применение увеличения выборки к данным\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# Применение уменьшения выборки к данным\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание экземпляра SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Применение SMOTE к данным\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cat_reg.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, pred)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(fscore)]\n",
    "print(f\"Best threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost\n",
    "pred = model.predict(X_val)\n",
    "# pred = [1 if x >= 0.5 else 0 for x in pred]\n",
    "print(classification_report(pred, y_val))\n",
    "print(f'F1_score: {f1_score(pred, y_val).round(3)}')\n",
    "print(f'Roc_auc: {roc_auc_score(pred, y_val).round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified cross-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat_classif\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Стратифицированная кросс-валидация F1-метрика (macro):\", cv_scores)\n",
    "print(\"Среднее значение F1:\", np.mean(cv_scores))\n",
    "print(\"Стандартное отклонение F1:\", np.std(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
